# -*- coding: utf-8 -*-
"""dl_q2v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LFuI9l1PizaMoMHfVnKmZ3tel4k3sDMT
"""

# pip install wandb
dataset = "fashion_mnist"
#-------------------------Implementing AgrParser-------------------------------------------------------
# python train.py -wp DeepLearning_Assignment1 -we cs22m081 -d fashion_mnist -e 5 -b 32 -l mean_squared_error -o adam -lr 0.001 -m 0.9 -beta 0.9 -beta1 0.9 -beta2 0.999 -w_d 0.0005 -w_i random -nhl 3 -sz 128 -a ReLU -cl 10
import argparse

parser = argparse.ArgumentParser(description="Stores all the hyperpamaters for the model.")
parser.add_argument("-wp", "--wandb_project",type=str, default="DeepLearning_Assignment1", help="Enter the Name of your Wandb Project")
parser.add_argument("-we", "--wandb_entity",type=str, default="cs22m081", help="Wandb Entity used to track experiments in the Weights & Biases dashboard.")
parser.add_argument("-d", "--dataset", default="fashion_mnist",type=str,choices=["mnist","fashion_mnist"])
parser.add_argument("-e", "--epochs",default="1", type=int, help="Number of epochs to train neural network.")
parser.add_argument("-b", "--batch_size",default="4", type=int, help="Batch size used to train neural network.")
parser.add_argument("-l", "--loss",default="cross_entropy", type=str,choices=["mean_squared_error", "cross_entropy"], help="Loss function to compute the loss.")
parser.add_argument("-o", "--optimizer",default="sgd", type=str, choices= ["sgd", "momentum", "nag", "rmsprop", "adam", "nadam"])
parser.add_argument("-lr", "--learning_rate",default="0.01", type=float, help="Learning rate used to optimize model parameters")
parser.add_argument("-m", "--momentum",default="0.04", type=float, help="Momentum used by momentum and nag optimizers.")
parser.add_argument("-beta", "--beta",default="0.5", type=float, help="Beta used by rmsprop optimizer")
parser.add_argument("-beta1", "--beta1",default="0.5", type=float, help="Beta1 used by adam and nadam optimizers.")
parser.add_argument("-beta2", "--beta2",default="0.5", type=float, help="Beta2 used by adam and nadam optimizers.")
parser.add_argument("-eps", "--epsilon",default="0.000001", type=float, help="Epsilon used by optimizers.")
parser.add_argument("-w_d", "--weight_decay",default="0.0", type=float, help="Weight decay used by optimizers.")
parser.add_argument("-w_i", "--weight_init",default="random", type=str,choices=["random", "Xavier"])
parser.add_argument("-nhl", "--num_layers",default="1", type=int, help="Number of hidden layers used in feedforward neural network.")
parser.add_argument("-sz", "--hidden_size",default="4", type=int, help="Number of hidden neurons in a feedforward layer.")
parser.add_argument("-cl", "--num_classes",default="10", type=int, help="Number of nuerons in the output layer.")
parser.add_argument("-a", "--activation",default="sigmoid", type=str, choices=["identity", "sigmoid", "tanh", "ReLU"])

args = parser.parse_args()

wandb_project = args.wandb_project
wandb_entity = args.wandb_entity
dataset = args.dataset
epochs = args.epochs
batch_size = args.batch_size
loss_func = args.loss
optimizer = args.optimizer
learning_rate = args.learning_rate
momentum = args.momentum
beta = args.beta
beta1 = args.beta1
beta2 = args.beta2
# epsilon = args.epsilon
lambd = args.weight_decay
wt_initialisation = args.weight_init
no_of_hidden_layers = args.num_layers
hidden_layer_size = args.hidden_size
activation_function = args.activation
no_of_classes = args.num_classes
# print("wandb_project :", wandb_project , "wandb_entity: ", wandb_entity,"dataset: ",dataset,"epochs: ",epochs,"batch_size: ",batch_size,"loss_func", loss_func,"optimizer",  optimizer ,"learning_rate: ", learning_rate,"momentum: ", momentum, "beta: ",beta, "beta1:", beta1,"beta2: ", beta2, "lambd: ", lambd,"no_of_hidden_layers:", no_of_hidden_layers, "activation_function: ", activation_function,"no_of_classes: ", no_of_classes)


import numpy as np 
import pandas as pd
import math
import matplotlib.pyplot as plt
import wandb
print("Loading Data...")
if dataset == "fashion_mnist":
  from keras.datasets import fashion_mnist
  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
else: 
  from keras.datasets import mnist
  (x_train, y_train), (x_test, y_test) = mnist.load_data()

from sklearn.model_selection import train_test_split
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)

print(f'{dataset.upper()} Dataset Loaded !!')
x_train = x_train / 255
x_test = x_test / 255
x_val = x_val / 255
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)
x_val = x_val.reshape(x_val.shape[0], -1)

#Global Variables
beta = 0.9
beta1 = 0.9
beta2 = 0.999
input_size = x_test.shape[1]


class NN: 
  # instantiate the weights and biases with random numbers. 
  def __init__(self, size, wt_initialisation): 
    self.W= [] 
    self.B = [] 
    self.preactivation = []
    self.activation = []
    if wt_initialisation.lower() == "xavier":
      for i in range(1, len(size)):
        n = np.sqrt(6 / (size[i] + size[i-1]))
        w = np.random.uniform(-n , n, (size[i], size[i-1]))
        b = np.random.uniform(-n , n, (size[i])) 
        self.W.append(w)
        self.B.append(b)
    # Random Initialisation
    else: 
      for i in range(1, len(size)):
        w = np.random.rand(size[i], size[i-1]) /(np.sqrt(size[i]))
        b = np.random.rand(size[i]) 
        self.W.append(w)
        self.B.append(b)


###############################################################
  def normalize(self, x):
    for i in range(x.shape[0]):
      argmax = np.argmax(x[i])
      argmin = np.argmin(x[i])
      maxval = x[i][argmax]
      minval = x[i][argmin]
      x[i] = (x[i] )/(maxval)
    return x
###############################################################

  def sigmoid(self, x): 
    # for i in range(x.shape[0]):
    #   argmax = np.argmax(x[i])
    #   maxval = x[i][argmax]
    #   x[i] = x[i] - maxval
    # for i in range(x.shape[0]): 
    #   for j in range(x.shape[1]):
    #     if x[i][j] < 0:
    #       x[i][j] = np.exp(x[i][j])/(1+np.exp(x[i][j]))
    #     else:
    #       x[i][j] = 1 / (1 + np.exp(-x[i][j]))
    # return x
    return 1/(1 + np.exp(-x))

###############################################################

  def sigmoid_derivative(self, x):
    sig = self.sigmoid(x)
    return sig*(1-sig)

###############################################################

  def relu(self, x): 
    return np.maximum(0, x)

###############################################################

  def relu_derivative(self, x):
    return np.max(np.sign(x),0)


###############################################################
    
  def tanh(self, x): 
    return np.tanh(x)

###############################################################

  def tanh_derivative(self, x):
    t = np.tanh(x)
    return 1 - t*t

###############################################################

  def softmax(self, x):
    # print("X",x)
    # x = self.normalize(x)
    for i in range(x.shape[0]):
      sum=0
      argmax = np.argmax(x[i])
      maxval = x[i][argmax]
      # x[i] = x[i] - maxval
      for j in range(x.shape[1]):
        sum=sum+np.exp(x[i][j]-maxval)
      # print("sum ",sum)
      x[i]=np.exp(x[i]-maxval)/sum
    # x = np.exp(x)/np.sum(np.exp(x), axis=0)
    # print("After",x)
    return x

###############################################################

  def softmax_derivative(self, x):
    v = self.softmax(x) 
    return v * (1 - v)

###############################################################

  def one_hot_encoded(self, y, size):
    return np.eye(size)[y]

###############################################################

  def cross_entropy(self, y_train, y_hat):
    loss=0
    for i in range (y_hat.shape[0]):
      loss+=-(np.log2(y_hat[i][y_train[i]]))
    return loss/y_hat.shape[0]


###############################################################

  def squared_error(self, y_train, y_hat, no_of_classes):
      loss = 0
      y_onehot = self.one_hot_encoded(y_train, no_of_classes)
      for i in range(y_hat.shape[0]):
          loss += np.sum((y_hat[i] - y_onehot[i])**2)
      return loss / y_train.shape[0]

###############################################################
  def loss_function(self, y_train, y_hat, no_of_classes, loss_func, lambd):
    loss = self.l2_regularize(lambd, y_train.shape[0])
    if loss_func.lower() == "cross_entropy":
      loss += self.cross_entropy(y_train, y_hat)
    else:
      loss += self.squared_error(y_train, y_hat, no_of_classes)
    return loss
    

###############################################################

  def hadmard_mul(self, a, b):
    c = np.array(np.zeros((a.shape[0],a.shape[1])))
    for i in range(a.shape[0]):
      for j in range(a.shape[1]):
        c[i][j] = a[i][j] * b[i][j]
    return c

###############################################################

  def l2_regularize(self, lambd, batch_size):
      acc = 0
      for i in range(len(self.W)):
        acc += np.sum(self.W[i] ** 2)
      return (lambd/(2.* batch_size)) * acc


###############################################################
  def batch_converter(self, x, y, batch_size):
    no_datapoints = x.shape[0]
    no_batches = no_datapoints // batch_size
    x_batch = []
    y_batch = []
    for i in range(no_batches):
      s = i*batch_size
      e = min((i+1)*batch_size , x.shape[0])
      x1 = np.array(x[s:e])
      y1 = np.array(y[s:e])
      x_batch.append(x1)
      y_batch.append(y1)
    # jo datapoints last me bach jayenge wo yaha pe add kr rhe
    if no_batches * batch_size < x_train.shape[0]:
      x1 = np.array(x_train[no_batches* batch_size :])
      y1 = np.array(y_train[no_batches* batch_size :])
      x_batch.append(x1)
      y_batch.append(y1)
    return x_batch, y_batch


###############################################################

  def forward(self, input, size, activation_function):
    # Calculating for the hiddlen layers
    for i in range(len(size)-2):
      Y = np.dot(input, self.W[i].T)   + self.B[i]
      if i == 0 and activation_function.lower() == "relu":
        Y= self.normalize(Y)
      elif activation_function.lower() == "tanh":
        Y = self.normalize(Y)
      if i < len(self.preactivation):
        self.preactivation[i] = Y
      else:
        self.preactivation.append(Y)
      # Y_dash = self.normalize(Y)
      if activation_function.lower() == "relu":
        Z = self.relu(Y)
      elif activation_function.lower() =="sigmoid":
        Z = self.sigmoid(Y)
      elif activation_function.lower() =="tanh":
        Z = self.tanh(Y)
      else:
        print("NO ACTIVATION FUNCTION SELECTED")

      if i < len(self.activation):
        self.activation[i] = Z
      else:
        self.activation.append(Z)
      input = Z
    #Calculating for the output layer.
    i =  len(size)-2
    Y = np.dot(input, self.W[i].T) + self.B[i]
    # Y = self.normalize(Y)
    if i < len(self.preactivation):
        self.preactivation[i] = Y
    else:
      self.preactivation.append(Y)
    Z = self.softmax(Y)
    if i < len(self.activation):
        self.activation[i] = Z
    else:
        self.activation.append(Z)
    return self.preactivation, self.activation

###############################################################


  def backward(self, layers, x, y ,no_of_classes, preac, ac, activation_function, loss_func):
    no_layers = len(layers)
    grad_a = [] 
    grad_w = []
    grad_b = []
    grad_h = []
    y_onehot = self.one_hot_encoded(y, no_of_classes)
    if loss_func.lower() == "cross_entropy":
      grad_a.append(-(y_onehot - ac[len(ac)-1]))
    else: #MSE
      grad_a.append((ac[len(ac)-1] - y_onehot) * self.softmax_derivative(ac[len(ac)-1]))#ac[len(ac)-1]*(1 - ac[len(ac)-1])) 
    # grad_a.append(-(y_onehot - ac[len(ac)-1]))

    for i in range(no_layers-2, -1, -1):
      if i == 0:
        dw = (grad_a[no_layers-2-i].T @ x) #/ y.shape[0]
        db = np.sum(grad_a[no_layers-2-i],axis=0)/y.shape[0]
      else: 
        dw = (grad_a[no_layers-2-i].T @ ac[i-1])#/ y.shape[0]
        db = np.sum(grad_a[no_layers-2-i],axis=0)/ y.shape[0]
        dh_1 = grad_a[no_layers-2-i] @ self.W[i]
        sig = 0 
        if activation_function.lower() == "sigmoid":
          sig = self.sigmoid_derivative(preac[i-1])
        elif activation_function.lower() == "relu":
          sig = self.relu_derivative(preac[i-1])
        elif activation_function.lower() == "tanh":
          sig = self.tanh_derivative(preac[i-1])
        else:
          print("CANNOT UPDATE WEIGHT AND BIASES !!")

        da_1 = dh_1 * sig

        grad_h.append(dh_1)
        grad_a.append(da_1)
      grad_w.append(dw)
      grad_b.append(db)
    return grad_w, grad_b


###############################################################


  def gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, eta, epochs, loss_func, lambd, do_wandb_log):
    l = len(layers)
    loss_arr = []
    for ep in range(epochs):
      preac, ac = self.forward(x_train, layers, activation_function)
      grad_w, grad_b = self.backward(layers, x_train, y_train, no_of_classes, preac, ac, activation_function, loss_func)

      for i in range(l-1):
        self.W[i] += -(eta * grad_w[l-i-2] + (eta * lambd * self.W[i])/self.W[i].shape[0])
        self.B[i] += -eta * grad_b[l-i-2]
      # print(ac[len(ac)-1])
      # preac, ac = self.forward(x, layers)
      preac, ac = self.forward(x_train, layers, activation_function)
      if loss_func.lower() == "cross_entropy":
        loss_train = self.cross_entropy(y_train, ac[len(ac)-1])
      else:
        loss_train = self.squared_error(y_train, ac[len(ac)-1], no_of_classes)
      preac, ac = self.forward(x_test, layers, activation_function)
      if loss_func.lower() == "cross_entropy":
        loss_val = self.cross_entropy(y_test, ac[len(ac)-1])
      else: 
        loss_val = self.squared_error(y_test, ac[len(ac)-1], no_of_classes)


      loss_arr.append(loss_val)
      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)
      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)
      print("Iteration No : \t\t", ep+1, "\t Train Loss\t\t", loss_train)
      print("Iteration No : \t\t", ep+1, "\t Validate Loss\t\t", loss_val)
      print("\n")
      print("Iteration No : \t\t", ep+1, "\t Train Accuracy\t\t", accur_train)
      print("Iteration No : \t\t", ep+1, "\t Validate Accuracy\t\t", accur_val)
      print("---------------------------------------------------------")
      if do_wandb_log == True:
        wandb.log({"train_accuracy":accur_train,"train_error":loss_train,"val_accuracy":accur_val,"val_error":loss_val})
      # print("Accuracy\t", accur, "%")
    # self.PlotError(loss_arr)



###############################################################


  def batch_grad_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, eta, batch_size, n_iterations, loss_func, lambd, do_wandb_log):
    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)
    loss_arr = []
    for i in range(n_iterations):
      for j in range(len(x_batch)):
        xb = x_batch[j]
        yb = y_batch[j]
        preac, ac = self.forward(xb, layers, activation_function)
        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes,preac, ac, activation_function, loss_func)
        length = len(layers)
        for l in range(length-1):
          # print("shape",self.W[l].shape, grad_w[length-l-2].shape)
          self.W[l] += -(eta * grad_w[length-l-2] + eta * lambd * self.W[l])
          self.B[l] += -eta * grad_b[length-l-2]
      preac, ac = self.forward(x_train, layers, activation_function)
      loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)
      
      preac, ac = self.forward(x_test, layers, activation_function )
      loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)

      loss_arr.append(loss_val)
      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)
      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)
      print("Iteration No : \t\t", i+1, "\t Train Loss\t\t", loss_train)
      print("Iteration No : \t\t", i+1, "\t Validate Loss\t\t", loss_val)
      print("\n")
      print("Iteration No : \t\t", i+1, "\t Train Accuracy\t\t", accur_train)
      print("Iteration No : \t\t", i+1, "\t Validate Accuracy\t\t", accur_val)
      print("---------------------------------------------------------")
      if do_wandb_log == True:
        wandb.log({"train_accuracy":accur_train,"train_error":loss_train,"val_accuracy":accur_val,"val_error":loss_val})
      # print("Accuracy\t", accur, "%")
    # self.PlotError(loss_arr)


###############################################################


  def momentum_grad_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta, loss_func,lambd, do_wandb_log):
    l = len(layers)
    prev_w = [] 
    prev_b = [] 
    loss_arr = []
    for i in range(l-1):
      prev_w.append(np.zeros(self.W[i].shape))
      prev_b.append(np.zeros(self.B[i].shape))

    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)
    for ep in range(epochs):
      for j in range(len(x_batch)):
        xb = x_batch[j]
        yb = y_batch[j]
        preac, ac = self.forward(xb, layers, activation_function)
        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)
        for i in range(l-1):
          prev_w[i] = beta*prev_w[i] + grad_w[l-i-2]
          prev_b[i] = beta*prev_b[i] + grad_b[l-i-2]
          self.W[i] += -(eta*prev_w[i] + eta * lambd * self.W[i])
          self.B[i] += -eta*prev_b[i]
      preac, ac = self.forward(x_train, layers, activation_function)
      loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)
      
      preac, ac = self.forward(x_test, layers, activation_function)
      loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)
      loss_arr.append(loss_val)
      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)
      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)

      print("Iteration No : \t\t", ep+1, "\t Train Loss\t\t", loss_train)
      print("Iteration No : \t\t", ep+1, "\t Validate Loss\t\t", loss_val)
      print("\n")
      print("Iteration No : \t\t", ep+1, "\t Train Accuracy\t\t", accur_train)
      print("Iteration No : \t\t", ep+1, "\t Validate Accuracy\t\t", accur_val)
      print("---------------------------------------------------------")
      
      if do_wandb_log == True:
        wandb.log({"train_accuracy":accur_train,"train_error":loss_train,"val_accuracy":accur_val,"val_error":loss_val})
      # print("Accuracy\t", accur, "%")
    # self.PlotError(loss_arr)


###############################################################

  def nesterov_gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta, loss_func,lambd, do_wandb_log):
    l = len(layers)
    prev_w = []
    prev_b = []
    loss_arr = []
    for i in range(l-1):
      prev_w.append(np.zeros(self.W[i].shape))
      prev_b.append(np.zeros(self.B[i].shape))

    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)
    
    for ep in range(epochs):
      for j in range(len(x_batch)):
        xb = x_batch[j]
        yb = y_batch[j]
        for i in range(l-1):
          self.W[i] += -beta * prev_w[i]
          self.B[i] += -beta * prev_b[i]
        preac, ac = self.forward(xb, layers, activation_function)
        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)
        # print("grad_w", grad_w)
        for i in range(l-1):
          prev_w[i] = beta * prev_w[i] + grad_w[l-i-2]
          prev_b[i] = beta * prev_b[i] + grad_b[l-i-2]
          self.W[i] += -(eta * prev_w[i] + eta * lambd * self.W[i])
          self.B[i] += -eta * prev_b[i]
      preac, ac = self.forward(x_train, layers, activation_function)
      loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)
      
      preac, ac = self.forward(x_test, layers, activation_function)
      loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)

      loss_arr.append(loss_val)
      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)
      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)
      print("Iteration No : \t\t", ep+1, "\t Train Loss\t\t", loss_train)
      print("Iteration No : \t\t", ep+1, "\t Validate Loss\t\t", loss_val)
      print("\n")
      print("Iteration No : \t\t", ep+1, "\t Train Accuracy\t\t", accur_train)
      print("Iteration No : \t\t", ep+1, "\t Validate Accuracy\t\t", accur_val)
      print("---------------------------------------------------------")
      if do_wandb_log == True:
        wandb.log({"train_accuracy":accur_train,"train_error":loss_train,"val_accuracy":accur_val,"val_error":loss_val})
      # print("Accuracy\t", accur, "%")
    # self.PlotError(loss_arr)



###############################################################


  def rmsprop_gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta, loss_func,lambd, do_wandb_log):
    l = len(layers)
    vw = []
    vb = []
    loss_arr = []
    eps = 1e-4
    for i in range(l-1):
      vw.append(np.zeros(self.W[i].shape))
      vb.append(np.zeros(self.B[i].shape))

    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)
    for ep in range(epochs):
      for j in range(len(x_batch)):
        xb = x_batch[j]
        yb = y_batch[j]
        preac, ac = self.forward(xb, layers, activation_function)
        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)
        for i in range(l-1):
          vw[i] = beta * vw[i] + (1-beta) * grad_w[l-i-2] * grad_w[l-i-2]
          vb[i] = beta * vb[i] + (1-beta) * grad_b[l-i-2] * grad_b[l-i-2]
          self.W[i] =  self.W[i] - (eta * grad_w[l-i-2])/np.sqrt(vw[i] + eps) - (eta * lambd * self.W[i])
          self.B[i] =  self.B[i] - (eta * grad_b[l-i-2])/np.sqrt(vb[i] + eps)
      preac, ac = self.forward(x_train, layers, activation_function)
      loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)

      preac, ac = self.forward(x_test, layers, activation_function)
      loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)
      
      loss_arr.append(loss_val)
      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)
      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)
      print("Iteration No : \t\t", ep+1, "\t Train Loss\t\t", loss_train)
      print("Iteration No : \t\t", ep+1, "\t Validate Loss\t\t", loss_val)
      print("\n")
      print("Iteration No : \t\t", ep+1, "\t Train Accuracy\t\t", accur_train)
      print("Iteration No : \t\t", ep+1, "\t Validate Accuracy\t\t", accur_val)
      print("---------------------------------------------------------")
      if do_wandb_log == True: 
        wandb.log({"train_accuracy":accur_train,"train_error":loss_train,"val_accuracy":accur_val,"val_error":loss_val})
      # print("Accuracy\t", accur, "%")
    # self.PlotError(loss_arr)


###############################################################


  def adam_gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta1, beta2, loss_func,lambd, do_wandb_log):
    l = len(layers)
    mw = []
    mb = []
    vw = []
    vb = []
    loss_arr = []
    eps = 1e-10
    for i in range(l-1):
      vw.append(np.zeros(self.W[i].shape))
      vb.append(np.zeros(self.B[i].shape))
      mw.append(np.zeros(self.W[i].shape))
      mb.append(np.zeros(self.B[i].shape))
    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)
    for ep in range(epochs):
      for j in range(len(x_batch)):
        xb = x_batch[j]
        yb = y_batch[j]
        preac, ac = self.forward(xb, layers, activation_function)
        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)
        for i in range(l-1):
          mw[i] = beta1 * mw[i] + (1-beta1) * grad_w[l-i-2]
          mb[i] = beta1 * mb[i] + (1-beta1) * grad_b[l-i-2]
          mw_hat = mw[i] / (1 - np.power(beta1, j+1))
          mb_hat = mb[i] / (1 - np.power(beta1, j+1))

          vw[i] = beta2 * vw[i] + (1-beta2) * grad_w[l-i-2] * grad_w[l-i-2]
          vb[i] = beta2 * vb[i] + (1-beta2) * grad_b[l-i-2] * grad_b[l-i-2]
          vw_hat = vw[i] / (1 - np.power(beta2, j+1))
          vb_hat = vb[i] / (1 - np.power(beta2, j+1))

          self.W[i] = self.W[i] - (eta * mw_hat)/(np.sqrt(vw_hat) + eps)- (eta * lambd * self.W[i])
          self.B[i] = self.B[i] - (eta * mb_hat)/(np.sqrt(vb_hat) + eps)
      preac, ac = self.forward(x_train, layers, activation_function)
      loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)

      preac, ac = self.forward(x_test, layers, activation_function)
      loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)

      loss_arr.append(loss_val)
      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)
      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)
      print("Iteration No : \t\t", ep+1, "\t Train Loss\t\t", loss_train)
      print("Iteration No : \t\t", ep+1, "\t Validate Loss\t\t", loss_val)
      print("\n")
      print("Iteration No : \t\t", ep+1, "\t Train Accuracy\t\t", accur_train)
      print("Iteration No : \t\t", ep+1, "\t Validate Accuracy\t\t", accur_val)
      print("---------------------------------------------------------")
      if do_wandb_log == True: 
        wandb.log({"train_accuracy":accur_train,"train_error":loss_train,"val_accuracy":accur_val,"val_error":loss_val})
      # print("Accuracy\t", accur, "%")
    # self.PlotError(loss_arr)

###############################################################


  def nadam_gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta1, beta2, loss_func,lambd, do_wandb_log):
    l = len(layers)
    mw = []
    mb = []
    vw = []
    vb = []
    loss_arr = []
    eps = 1e-10
    for i in range(l-1):
      vw.append(np.zeros(self.W[i].shape))
      vb.append(np.zeros(self.B[i].shape))
      mw.append(np.zeros(self.W[i].shape))
      mb.append(np.zeros(self.B[i].shape))
    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)
    for ep in range(epochs):
      for j in range(len(x_batch)):
        xb = x_batch[j]
        yb = y_batch[j]
        preac, ac = self.forward(xb, layers, activation_function)
        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)
        for i in range(l-1): 
          mw[i] = beta1 * mw[i] + (1-beta1)* grad_w[l-i-2]
          mb[i] = beta1 * mb[i] + (1-beta1)* grad_b[l-i-2]
          mw_hat = mw[i] / (1 - np.power(beta1, j+1))
          mb_hat = mb[i] / (1 - np.power(beta1, j+1))

          vw[i] = beta2 * vw[i] + (1-beta2) * grad_w[l-i-2] * grad_w[l-i-2]
          vb[i] = beta2 * vb[i] + (1-beta2) * grad_b[l-i-2] * grad_b[l-i-2]
          vw_hat = vw[i] / (1 - np.power(beta2, j+1))
          vb_hat = vb[i] / (1 - np.power(beta2, j+1))

          self.W[i] = self.W[i] - (eta/(np.sqrt(vw[i]) + eps)) * (beta1 * mw_hat + (((1-beta1) * grad_w[l-i-2]) / (1 - np.power(beta1, j+1)))) - (eta * lambd * self.W[i])
          self.B[i] = self.B[i] - (eta/(np.sqrt(vb[i]) + eps)) * (beta1 * mb_hat + (((1-beta1) * grad_b[l-i-2]) / (1 - np.power(beta1, j+1))))
      preac, ac = self.forward(x_train, layers, activation_function)
      loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)

      preac, ac = self.forward(x_test, layers, activation_function)
      loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)

      loss_arr.append(loss_val)
      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)
      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)
      print("Iteration No : \t\t", ep+1, "\t Train Loss\t\t", loss_train)
      print("Iteration No : \t\t", ep+1, "\t Validate Loss\t\t", loss_val)
      print("\n")
      print("Iteration No : \t\t", ep+1, "\t Train Accuracy\t\t", accur_train)
      print("Iteration No : \t\t", ep+1, "\t Validate Accuracy\t\t", accur_val)
      print("---------------------------------------------------------")
      if do_wandb_log == True:
        wandb.log({"train_accuracy":accur_train,"train_error":loss_train,"val_accuracy":accur_val,"val_error":loss_val})
    #   print("Accuracy\t", accur, "%")
    # self.PlotError(loss_arr)
    

###############################################################

  def test_accuracy(self, layers, x, y, activation_function):
    preac, ac = self.forward(x, layers, activation_function)
    y_pred = ac[len(ac)-1]
    err_count = 0
    for i in range(y_pred.shape[0]):
      maxval = -(math.inf)
      maxind= -1
      for j in range(y_pred.shape[1]): 
        if maxval < y_pred[i][j]:
          maxval = y_pred[i][j]
          maxind = j
      if maxind != y[i]:
        err_count = err_count + 1
    return ((y.shape[0] - err_count)/y.shape[0])*100
      
###############################################################
  def PlotError(self, ErrorSum):
    Iter = [] 
    for i in range(len(ErrorSum)):
      Iter.append(i)
    plt.plot(Iter,ErrorSum)
    plt.title('Error v/s Iteration')
    plt.xlabel('No of Iterations')
    plt.ylabel('Error')
    plt.show()

def main(x_train, y_train, x_val, y_val, input_size, no_hidden_layers, hidden_layer_size, no_of_classes, wt_initialisation, optimiser, activation_function, batch_size, eta, epoch, momentum, beta, beta1, beta2, loss_func, lambd, do_wandb_log):
    layers = []
    layers.append(input_size)
    for i in range(no_hidden_layers):
      layers.append(hidden_layer_size)
    layers.append(no_of_classes)
     
    nn = NN(layers, wt_initialisation)
    if optimiser.lower() == "sgd":
      nn.batch_grad_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, eta, batch_size, epoch, loss_func, lambd, do_wandb_log)
    elif optimiser.lower() == "vanillagd":
      nn.gradient_descent(x_train, y_train,x_val, y_val, no_of_classes, layers, activation_function, eta, epoch, loss_func, lambd, do_wandb_log)
    elif optimiser.lower() == "momentum":
      nn.momentum_grad_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, momentum, loss_func, lambd, do_wandb_log)
    elif optimiser.lower() == "nag":
      nn.nesterov_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, momentum, loss_func, lambd, do_wandb_log)

    elif optimiser.lower() == "rmsprop":
      nn.rmsprop_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, beta, loss_func, lambd, do_wandb_log)
      
    elif optimiser.lower() == "adam":
      nn.adam_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, beta1, beta2, loss_func, lambd, do_wandb_log)

    elif optimiser.lower() == "nadam":
      nn.nadam_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, beta1, beta2, loss_func, lambd, do_wandb_log)



# To run the Sweep set do_wandb_log = True
# To run it using command set do_wandb_log = False 

# If do_wandb_log = False, To run using command set run_from_command = True else Hardcode the values from Line No 812.
do_wandb_log = True
run_from_command = True


if do_wandb_log == True:

  default_params=dict(
  epochs=10,
  batch_size=32,
  input_size = 784,
  optimizer='nadam',
  learning_rate=0.001,
  loss_func = "cross_entropy",
  activation_function='sigmoid',
  no_of_classes = 10,
  no_of_hidden_layers=3,
  hidden_layer_size=128,
  wt_initialisation='Xavier',
  lambd = 0, 
  momentum = 0.05,
  )
  run=wandb.init(config=default_params,project=wandb_project,entity=wandb_entity,reinit='true')
  config=wandb.config

  epochs=config.epochs
  batch_size=config.batch_size
  optimizer=config.optimizer
  learning_rate = config.learning_rate
  loss_func = config.loss_func
  no_of_hidden_layers=config.no_of_hidden_layers
  hidden_layer_size=config.hidden_layer_size
  wt_initialisation=config.wt_initialisation
  input_size=config.input_size
  activation_function = config.activation_function
  no_of_classes = config.no_of_classes
  lambd = config.lambd
  momentum = config.momentum

  run.name='hl_'+str(no_of_hidden_layers)+'_bs_'+str(batch_size)+'_ac_'+activation_function+'_op_'+optimizer+'_l_'+loss_func+'_wd_'+str(lambd)+'_m_'+str(momentum)
  print("epochs: ",epochs,"| batch_size: ",batch_size,"| loss_func", loss_func,"| optimizer",  optimizer ,"| learning_rate: ", learning_rate,"| momentum", momentum, "| beta: ",beta, "| beta1:", beta1,"| beta2: ", beta2, "| lambd: ", lambd,"| no_of_hidden_layers:", no_of_hidden_layers, "| activation_function: ", activation_function,"| no_of_classes: ", no_of_classes)
  main(x_train, y_train, x_val, y_val, input_size, no_of_hidden_layers, hidden_layer_size, no_of_classes, wt_initialisation, optimizer, activation_function, batch_size, learning_rate, epochs, momentum, beta, beta1, beta2, loss_func, lambd, do_wandb_log)

else:
  if run_from_command == False:
    #Set the Require Parameters here
    epochs = 4
    batch_size = 1024
    input_size = 784
    # "sgd", "momentum", "nag", "rmsprop", "adam", "nadam"
    optimizer = 'nag'
    learning_rate = 0.001
    beta = 0.01
    # "mean_squared_error", "cross_entropy"
    loss_func = "mse"
    # "identity", "sigmoid", "tanh", "ReLU"
    activation_function ='tanh'
    no_of_classes = 10
    no_of_hidden_layers = 3
    hidden_layer_size = 128
    # "random", "Xavier"
    wt_initialisation = 'Xavier'
    lambd = 0
    momentum = 0.05
  print("epochs: ",epochs,"| batch_size: ",batch_size,"| loss_func", loss_func,"| optimizer",  optimizer ,"| learning_rate: ", learning_rate,"| momentum", momentum, "| beta: ",beta, "| beta1:", beta1,"| beta2: ", beta2, "| lambd: ", lambd,"| no_of_hidden_layers:", no_of_hidden_layers, "| activation_function: ", activation_function,"| no_of_classes: ", no_of_classes)
  main(x_train, y_train, x_val, y_val, input_size, no_of_hidden_layers, hidden_layer_size, no_of_classes, wt_initialisation, optimizer, activation_function, batch_size, learning_rate, epochs, momentum, beta, beta1, beta2, loss_func, lambd, do_wandb_log)


  


