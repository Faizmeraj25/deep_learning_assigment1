{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI910qtq1B1q+Vc48tivgw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Faizmeraj25/deep_learning_assigment1/blob/main/dl_q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6m6A_sxA1dD"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "import tensorflow as tf\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "Xz9hOc4JBc2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be21e30d-0522-490c-9ca8-63e5cba92278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NN: \n",
        "  # instantiate the weights and biases with random numbers. \n",
        "  def __init__(self, size): \n",
        "    self.W= [] \n",
        "    self.B = [] \n",
        "    self.preactivation = []\n",
        "    self.activation = []\n",
        "    for i in range(1, len(size)):\n",
        "      w = np.random.rand(size[i], size[i-1]) /(np.sqrt(size[i]))\n",
        "      b = np.random.rand( size[i]) \n",
        "      self.W.append(w)\n",
        "      self.B.append(b)\n",
        "\n",
        "###############################################################\n",
        "  def normalize(self, x):\n",
        "    for i in range(x.shape[0]):\n",
        "      argmax = np.argmax(x[i])\n",
        "      maxval = x[i][argmax]\n",
        "      x[i] = (x[i])/(maxval)\n",
        "    return x\n",
        "###############################################################\n",
        "\n",
        "  def sigmoid(self, x): \n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "###############################################################\n",
        "\n",
        "  def sigmoid_derivative(self, x):\n",
        "    sig = self.sigmoid(x)\n",
        "    return sig*(1-sig)\n",
        "\n",
        "###############################################################\n",
        "\n",
        "  def softmax(self, x):\n",
        "    for i in range(x.shape[0]):\n",
        "      sum=0\n",
        "      for j in range(x.shape[1]):\n",
        "        sum=sum+np.exp(x[i][j])\n",
        "      x[i]=np.exp(x[i])/sum\n",
        "    return x\n",
        "\n",
        "###############################################################\n",
        "\n",
        "  def one_hot_encoded(self, y, size):\n",
        "    return np.eye(size)[y]\n",
        "\n",
        "###############################################################\n",
        "\n",
        "  def cross_entropy(self, y_train, y_hat):\n",
        "    loss=0\n",
        "    for i in range (y_hat.shape[0]):\n",
        "      loss+=-(np.log2(y_hat[i][y_train[i]]))\n",
        "    return loss/y_hat.shape[0]\n",
        "\n",
        "###############################################################\n",
        "\n",
        "  def hadmard_mul(self, a, b):\n",
        "    c = np.array(np.zeros((a.shape[0],a.shape[1])))\n",
        "    for i in range(a.shape[0]):\n",
        "      for j in range(a.shape[1]):\n",
        "        c[i][j] = a[i][j] * b[i][j]\n",
        "    return c\n",
        "\n",
        "###############################################################\n",
        "  def batch_converter(self, x, y, batch_size):\n",
        "    no_datapoints = x.shape[0]\n",
        "    no_batches = no_datapoints // batch_size\n",
        "    x_batch = []\n",
        "    y_batch = []\n",
        "    for i in range(no_batches):\n",
        "      s = i*batch_size\n",
        "      e = min((i+1)*batch_size , x.shape[0])\n",
        "      x1 = np.array(x[s:e])\n",
        "      y1 = np.array(y[s:e])\n",
        "      x_batch.append(x1)\n",
        "      y_batch.append(y1)\n",
        "    # jo datapoints last me bach jayenge wo yaha pe add kr rhe\n",
        "    if no_batches * batch_size < x_train.shape[0]:\n",
        "      x1 = np.array(x_train[no_batches* batch_size :])\n",
        "      y1 = np.array(y_train[no_batches* batch_size :])\n",
        "      x_batch.append(x1)\n",
        "      y_batch.append(y1)\n",
        "    return x_batch, y_batch\n",
        "\n",
        "\n",
        "###############################################################\n",
        "\n",
        "  def forward(self, input, size):\n",
        "    # Calculating for the hiddlen layers\n",
        "    for i in range(len(size)-2):\n",
        "      Y = np.dot(input, self.W[i].T)   + self.B[i]\n",
        "      # Y= self.normalize(Y)\n",
        "      if i < len(self.preactivation):\n",
        "        self.preactivation[i] = Y\n",
        "      else:\n",
        "        self.preactivation.append(Y)\n",
        "      # Y_dash = self.normalize(Y)\n",
        "      Z = self.sigmoid(Y)\n",
        "      if i < len(self.activation):\n",
        "        self.activation[i] = Z\n",
        "      else:\n",
        "        self.activation.append(Z)\n",
        "      input = Z\n",
        "    #Calculating for the output layer.\n",
        "    i =  len(size)-2\n",
        "    Y = np.dot(input, self.W[i].T) + self.B[i]\n",
        "    # Y = self.normalize(Y)\n",
        "    if i < len(self.preactivation):\n",
        "        self.preactivation[i] = Y\n",
        "    else:\n",
        "      self.preactivation.append(Y)\n",
        "    Z = self.softmax(Y)\n",
        "    if i < len(self.activation):\n",
        "        self.activation[i] = Z\n",
        "    else:\n",
        "        self.activation.append(Z)\n",
        "    return self.preactivation, self.activation\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "  def backward(self, layers, x, y ,no_of_classes, preac, ac):\n",
        "    no_layers = len(layers)\n",
        "    grad_a = [] \n",
        "    grad_w = []\n",
        "    grad_b = []\n",
        "    grad_h = []\n",
        "    y_onehot = self.one_hot_encoded(y, no_of_classes)\n",
        "    grad_a.append(-(y_onehot - ac[len(ac)-1]))\n",
        "\n",
        "    for i in range(no_layers-2, -1, -1):\n",
        "      if i == 0:\n",
        "        dw = (grad_a[no_layers-2-i].T @ x) #/ y.shape[0]\n",
        "        db = np.sum(grad_a[no_layers-2-i],axis=0)/y.shape[0]\n",
        "      else: \n",
        "        dw = (grad_a[no_layers-2-i].T @ ac[i-1])#/ y.shape[0]\n",
        "        db = np.sum(grad_a[no_layers-2-i],axis=0)/ y.shape[0]\n",
        "        dh_1 = grad_a[no_layers-2-i] @ self.W[i]\n",
        "        sig = self.sigmoid_derivative(preac[i-1])\n",
        "        da_1 = dh_1 * sig\n",
        "\n",
        "        grad_h.append(dh_1)\n",
        "        grad_a.append(da_1)\n",
        "      grad_w.append(dw)\n",
        "      grad_b.append(db)\n",
        "    return grad_w, grad_b\n",
        "\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "  def gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, eta, epochs):\n",
        "    l = len(layers)\n",
        "    for ep in range(epochs):\n",
        "      preac, ac = self.forward(x_train, layers)\n",
        "      grad_w, grad_b = self.backward(layers, x_train, y_train, no_of_classes, preac, ac)\n",
        "\n",
        "      for i in range(l-1):\n",
        "        self.W[i] += -eta * grad_w[l-i-2]\n",
        "        self.B[i] += -eta * grad_b[l-i-2]\n",
        "      # print(ac[len(ac)-1])\n",
        "      # preac, ac = self.forward(x, layers)\n",
        "      loss = self.cross_entropy(y_train, ac[len(ac)-1])\n",
        "      print(\"Iteration No : \\t\", ep+1, \"\\t Loss\\t\", loss)\n",
        "    accur = self.test_accuracy(layers, x_test, y_test)\n",
        "    print(\"Accuracy\\t\", accur, \"%\")\n",
        "\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "  def batch_grad_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, eta, batch_size, n_iterations):\n",
        "    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "      for j in range(len(x_batch)):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        preac, ac = self.forward(xb, layers)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes,preac, ac)\n",
        "        length = len(layers)\n",
        "        for l in range(length-1):\n",
        "          # print(\"shape\",self.W[l].shape, grad_w[length-l-2].shape)\n",
        "          self.W[l] += -eta * grad_w[length-l-2]\n",
        "          self.B[l] += -eta * grad_b[length-l-2]\n",
        "      loss = self.cross_entropy(yb, ac[len(ac)-1])\n",
        "      print(\"Iteration No : \\t\", i+1, \"\\t Loss\\t\", loss)\n",
        "    accur = self.test_accuracy(layers, x_test, y_test)\n",
        "    print(\"Accuracy\\t\",accur, \"%\")\n",
        "\n",
        "\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "  def momentum_grad_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, batch_size, eta, epochs, beta):\n",
        "    l = len(layers)\n",
        "    prev_w = [] \n",
        "    prev_b = [] \n",
        "    for i in range(l-1):\n",
        "      prev_w.append(np.zeros(self.W[i].shape))\n",
        "      prev_b.append(np.zeros(self.B[i].shape))\n",
        "\n",
        "    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)\n",
        "    for ep in range(epochs):\n",
        "      for j in range(len(x_batch)):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        preac, ac = self.forward(xb, layers)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac)\n",
        "        for i in range(l-1):\n",
        "          uw = beta*prev_w[i] + grad_w[l-i-2]\n",
        "          ub = beta*prev_b[i] + grad_b[l-i-2]\n",
        "          self.W[i] += -eta*uw\n",
        "          self.B[i] += -eta*ub\n",
        "          prev_w[i] = self.W[i]\n",
        "          prev_b[i] = self.B[i]\n",
        "      loss = self.cross_entropy(yb, ac[len(ac)-1])\n",
        "      print(\"Iteration No : \\t\", ep+1, \"\\t Loss\\t\", loss)\n",
        "    accur = self.test_accuracy(layers, x_test, y_test)\n",
        "    print(\"Accuracy\\t\",accur, \"%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "  def test_accuracy(self, layers, x, y):\n",
        "    preac, ac = self.forward(x, layers)\n",
        "    y_pred = ac[len(ac)-1]\n",
        "    err_count = 0\n",
        "    for i in range(y_pred.shape[0]):\n",
        "      maxval = -(math.inf)\n",
        "      maxind= -1\n",
        "      for j in range(y_pred.shape[1]): \n",
        "        if maxval < y_pred[i][j]:\n",
        "          maxval = y_pred[i][j]\n",
        "          maxind = j\n",
        "      if maxind != y[i]:\n",
        "        err_count = err_count + 1\n",
        "    return ((y.shape[0] - err_count)/y.shape[0])*100\n",
        "      \n",
        "###############################################################"
      ],
      "metadata": {
        "id": "eElQPRNABGNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch Gradient Descent"
      ],
      "metadata": {
        "id": "RbhfxkM7dyRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 10\n",
        "eta = 0.001\n",
        "layers = [784, 128, 10]\n",
        "no_of_classes = 10\n",
        "nn = NN(layers)\n",
        "batch_size = 60\n",
        "nn.batch_grad_descent(x_train, y_train, x_test, y_test, no_of_classes, layers, eta, batch_size, epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxrVj3Aw9VH0",
        "outputId": "ff96c7cc-78d3-4e5c-ffac-1955edb05ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No : \t 1 \t Loss\t 3.3526490478748183\n",
            "Iteration No : \t 2 \t Loss\t 2.9390140445871986\n",
            "Iteration No : \t 3 \t Loss\t 2.4684183580400387\n",
            "Iteration No : \t 4 \t Loss\t 1.808689944845171\n",
            "Iteration No : \t 5 \t Loss\t 1.4716088237176812\n",
            "Iteration No : \t 6 \t Loss\t 1.325318135022857\n",
            "Iteration No : \t 7 \t Loss\t 1.1527154317057284\n",
            "Iteration No : \t 8 \t Loss\t 1.0653949780761063\n",
            "Iteration No : \t 9 \t Loss\t 0.980922395533707\n",
            "Iteration No : \t 10 \t Loss\t 0.9006248936889284\n",
            "Accuracy\t 80.21000000000001 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla Gradient Descent"
      ],
      "metadata": {
        "id": "AlEYWCg0mtLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 20\n",
        "eta = 0.0000009\n",
        "layers = [784, 128, 10]\n",
        "no_of_classes = 10\n",
        "nn = NN(layers)\n",
        "nn.gradient_descent(x_train, y_train,x_test, y_test, no_of_classes, layers, eta, epoch)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAnDgkWYWC-d",
        "outputId": "e7b56a5e-a254-435c-acc9-377f2fab91cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No : \t 1 \t Loss\t 5.10578270321741\n",
            "Iteration No : \t 2 \t Loss\t 3.8258173576837664\n",
            "Iteration No : \t 3 \t Loss\t 3.4119897243670496\n",
            "Iteration No : \t 4 \t Loss\t 3.3393824212093173\n",
            "Iteration No : \t 5 \t Loss\t 3.3238051723542075\n",
            "Iteration No : \t 6 \t Loss\t 3.320968766345521\n",
            "Iteration No : \t 7 \t Loss\t 3.32058023434749\n",
            "Iteration No : \t 8 \t Loss\t 3.3205321963855954\n",
            "Iteration No : \t 9 \t Loss\t 3.3205223374482653\n",
            "Iteration No : \t 10 \t Loss\t 3.320516372036673\n",
            "Iteration No : \t 11 \t Loss\t 3.3205107899388384\n",
            "Iteration No : \t 12 \t Loss\t 3.3205052445359295\n",
            "Iteration No : \t 13 \t Loss\t 3.3204997019953018\n",
            "Iteration No : \t 14 \t Loss\t 3.3204941590222647\n",
            "Iteration No : \t 15 \t Loss\t 3.3204886152918993\n",
            "Iteration No : \t 16 \t Loss\t 3.320483070767847\n",
            "Iteration No : \t 17 \t Loss\t 3.3204775254418473\n",
            "Iteration No : \t 18 \t Loss\t 3.320471979307969\n",
            "Iteration No : \t 19 \t Loss\t 3.320466432360982\n",
            "Iteration No : \t 20 \t Loss\t 3.3204608845954566\n",
            "Accuracy\t 9.969999999999999 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Momentum Gradient Descent"
      ],
      "metadata": {
        "id": "4u_RpwPLz8D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 10\n",
        "eta = 0.006\n",
        "beta = 0.9\n",
        "layers = [784, 128, 10]\n",
        "no_of_classes = 10\n",
        "batch_size = 1024\n",
        "nn = NN(layers)\n",
        "nn.momentum_grad_descent(x_train, y_train, x_test, y_test, no_of_classes, layers, batch_size, eta, epoch, beta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4nDyLrYsbmR",
        "outputId": "cf97e30d-6234-4de7-a632-532524f44a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No : \t 1 \t Loss\t 3.004217369017185\n",
            "Iteration No : \t 2 \t Loss\t 2.954323814937206\n",
            "Iteration No : \t 3 \t Loss\t 2.9309312597853006\n",
            "Iteration No : \t 4 \t Loss\t 2.8987458735893643\n",
            "Iteration No : \t 5 \t Loss\t 2.2986268114529755\n",
            "Iteration No : \t 6 \t Loss\t 1.349956004814287\n",
            "Iteration No : \t 7 \t Loss\t 1.0527738289226396\n",
            "Iteration No : \t 8 \t Loss\t 1.1987804634652246\n",
            "Iteration No : \t 9 \t Loss\t 1.0018224990789297\n",
            "Iteration No : \t 10 \t Loss\t 1.0527431490912378\n",
            "Accuracy\t 73.34 %\n"
          ]
        }
      ]
    }
  ]
}