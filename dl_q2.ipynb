{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOf4EcJomSWBn4UkbXdyBno",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Faizmeraj25/deep_learning_assigment1/blob/main/dl_q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6m6A_sxA1dD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70bd21b-7993-4bb6-9e4d-2a57865796be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "import tensorflow as tf\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "Xz9hOc4JBc2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac4fc75-74a8-4928-de4e-82612f93e52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NN: \n",
        "  # instantiate the weights and biases with random numbers. \n",
        "  def __init__(self, size): \n",
        "    self.W= [] \n",
        "    self.B = [] \n",
        "    self.preactivation = []\n",
        "    self.activation = []\n",
        "    for i in range(1, len(size)):\n",
        "      w = np.random.rand(size[i], size[i-1]) /(np.sqrt(size[i]))\n",
        "      b = np.random.rand( size[i]) \n",
        "      self.W.append(w)\n",
        "      self.B.append(b)\n",
        "\n",
        "###############################################################\n",
        "  def normalize(self, x):\n",
        "    for i in range(x.shape[0]):\n",
        "      argmax = np.argmax(x[i])\n",
        "      maxval = x[i][argmax]\n",
        "      x[i] = (x[i])/(maxval)\n",
        "    return x\n",
        "###############################################################\n",
        "\n",
        "  def sigmoid(self, x): \n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "###############################################################\n",
        "\n",
        "  def sigmoid_derivative(self, x):\n",
        "    sig = self.sigmoid(x)\n",
        "    return sig*(1-sig)\n",
        "\n",
        "###############################################################\n",
        "\n",
        "  def softmax(self, x):\n",
        "    for i in range(x.shape[0]):\n",
        "      sum=0\n",
        "      for j in range(x.shape[1]):\n",
        "        sum=sum+np.exp(x[i][j])\n",
        "      x[i]=np.exp(x[i])/sum\n",
        "    return x\n",
        "\n",
        "###############################################################\n",
        "\n",
        "  def one_hot_encoded(self, y, size):\n",
        "    return np.eye(size)[y]\n",
        "\n",
        "###############################################################\n",
        "\n",
        "  def cross_entropy(self, y_train, y_hat):\n",
        "    loss=0\n",
        "    for i in range (y_hat.shape[0]):\n",
        "      loss+=-(np.log2(y_hat[i][y_train[i]]))\n",
        "    return loss/y_hat.shape[0]\n",
        "\n",
        "###############################################################\n",
        "\n",
        "  def hadmard_mul(self, a, b):\n",
        "    c = np.array(np.zeros((a.shape[0],a.shape[1])))\n",
        "    for i in range(a.shape[0]):\n",
        "      for j in range(a.shape[1]):\n",
        "        c[i][j] = a[i][j] * b[i][j]\n",
        "    return c\n",
        "\n",
        "###############################################################\n",
        "  def batch_converter(self, x, y, batch_size):\n",
        "    no_datapoints = x.shape[0]\n",
        "    no_batches = no_datapoints // batch_size\n",
        "    x_batch = []\n",
        "    y_batch = []\n",
        "    for i in range(no_batches):\n",
        "      s = i*batch_size\n",
        "      e = min((i+1)*batch_size , x.shape[0])\n",
        "      x1 = np.array(x[s:e])\n",
        "      y1 = np.array(y[s:e])\n",
        "      x_batch.append(x1)\n",
        "      y_batch.append(y1)\n",
        "    # jo datapoints last me bach jayenge wo yaha pe add kr rhe\n",
        "    if no_batches * batch_size < x_train.shape[0]:\n",
        "      x1 = np.array(x_train[no_batches* batch_size :])\n",
        "      y1 = np.array(y_train[no_batches* batch_size :])\n",
        "      x_batch.append(x1)\n",
        "      y_batch.append(y1)\n",
        "    return x_batch, y_batch\n",
        "\n",
        "\n",
        "###############################################################\n",
        "\n",
        "  def forward(self, input, size):\n",
        "    # Calculating for the hiddlen layers\n",
        "    for i in range(len(size)-2):\n",
        "      Y = np.dot(input, self.W[i].T)   + self.B[i]\n",
        "      # Y= self.normalize(Y)\n",
        "      if i < len(self.preactivation):\n",
        "        self.preactivation[i] = Y\n",
        "      else:\n",
        "        self.preactivation.append(Y)\n",
        "      # Y_dash = self.normalize(Y)\n",
        "      Z = self.sigmoid(Y)\n",
        "      if i < len(self.activation):\n",
        "        self.activation[i] = Z\n",
        "      else:\n",
        "        self.activation.append(Z)\n",
        "      input = Z\n",
        "    #Calculating for the output layer.\n",
        "    i =  len(size)-2\n",
        "    Y = np.dot(input, self.W[i].T) + self.B[i]\n",
        "    # Y = self.normalize(Y)\n",
        "    if i < len(self.preactivation):\n",
        "        self.preactivation[i] = Y\n",
        "    else:\n",
        "      self.preactivation.append(Y)\n",
        "    Z = self.softmax(Y)\n",
        "    if i < len(self.activation):\n",
        "        self.activation[i] = Z\n",
        "    else:\n",
        "        self.activation.append(Z)\n",
        "    return self.preactivation, self.activation\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "  def backward(self, layers, x, y ,no_of_classes, preac, ac):\n",
        "    no_layers = len(layers)\n",
        "    grad_a = [] \n",
        "    grad_w = []\n",
        "    grad_b = []\n",
        "    grad_h = []\n",
        "    y_onehot = self.one_hot_encoded(y, no_of_classes)\n",
        "    grad_a.append(-(y_onehot - ac[len(ac)-1]))\n",
        "\n",
        "    for i in range(no_layers-2, -1, -1):\n",
        "      if i == 0:\n",
        "        dw = (grad_a[no_layers-2-i].T @ x) #/ y.shape[0]\n",
        "        db = np.sum(grad_a[no_layers-2-i],axis=0)/y.shape[0]\n",
        "      else: \n",
        "        dw = (grad_a[no_layers-2-i].T @ ac[i-1])#/ y.shape[0]\n",
        "        db = np.sum(grad_a[no_layers-2-i],axis=0)/ y.shape[0]\n",
        "        dh_1 = grad_a[no_layers-2-i] @ self.W[i]\n",
        "        sig = self.sigmoid_derivative(preac[i-1])\n",
        "        da_1 = dh_1 * sig\n",
        "\n",
        "        grad_h.append(dh_1)\n",
        "        grad_a.append(da_1)\n",
        "      grad_w.append(dw)\n",
        "      grad_b.append(db)\n",
        "    return grad_w, grad_b\n",
        "\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "  def gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, eta, epochs):\n",
        "    l = len(layers)\n",
        "    loss_arr = []\n",
        "    for ep in range(epochs):\n",
        "      preac, ac = self.forward(x_train, layers)\n",
        "      grad_w, grad_b = self.backward(layers, x_train, y_train, no_of_classes, preac, ac)\n",
        "\n",
        "      for i in range(l-1):\n",
        "        self.W[i] += -eta * grad_w[l-i-2]\n",
        "        self.B[i] += -eta * grad_b[l-i-2]\n",
        "      # print(ac[len(ac)-1])\n",
        "      # preac, ac = self.forward(x, layers)\n",
        "      loss = self.cross_entropy(y_train, ac[len(ac)-1])\n",
        "      print(\"Iteration No : \\t\", ep+1, \"\\t Loss\\t\", loss)\n",
        "      loss_arr.append(loss)\n",
        "    accur = self.test_accuracy(layers, x_test, y_test)\n",
        "    print(\"Accuracy\\t\", accur, \"%\")\n",
        "    self.PlotError(loss_arr)\n",
        "\n",
        "\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "  def batch_grad_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, eta, batch_size, n_iterations):\n",
        "    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)\n",
        "    loss_arr = []\n",
        "    for i in range(n_iterations):\n",
        "      for j in range(len(x_batch)):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        preac, ac = self.forward(xb, layers)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes,preac, ac)\n",
        "        length = len(layers)\n",
        "        for l in range(length-1):\n",
        "          # print(\"shape\",self.W[l].shape, grad_w[length-l-2].shape)\n",
        "          self.W[l] += -eta * grad_w[length-l-2]\n",
        "          self.B[l] += -eta * grad_b[length-l-2]\n",
        "      loss = self.cross_entropy(yb, ac[len(ac)-1])\n",
        "      print(\"Iteration No : \\t\", i+1, \"\\t Loss\\t\", loss)\n",
        "      loss_arr.append(loss)\n",
        "    accur = self.test_accuracy(layers, x_test, y_test)\n",
        "    print(\"Accuracy\\t\",accur, \"%\")\n",
        "    self.PlotError(loss_arr)\n",
        "\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "  def momentum_grad_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, batch_size, eta, epochs, beta):\n",
        "    l = len(layers)\n",
        "    prev_w = [] \n",
        "    prev_b = [] \n",
        "    loss_arr = []\n",
        "    for i in range(l-1):\n",
        "      prev_w.append(np.zeros(self.W[i].shape))\n",
        "      prev_b.append(np.zeros(self.B[i].shape))\n",
        "\n",
        "    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)\n",
        "    for ep in range(epochs):\n",
        "      for j in range(len(x_batch)):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        preac, ac = self.forward(xb, layers)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac)\n",
        "        for i in range(l-1):\n",
        "          prev_w[i] = beta*prev_w[i] + grad_w[l-i-2]\n",
        "          prev_b[i] = beta*prev_b[i] + grad_b[l-i-2]\n",
        "          self.W[i] += -eta*prev_w[i]\n",
        "          self.B[i] += -eta*prev_b[i]\n",
        "          # prev_w[i] = self.W[i]\n",
        "          # prev_b[i] = self.B[i]\n",
        "      loss = self.cross_entropy(yb, ac[len(ac)-1])\n",
        "      print(\"Iteration No : \\t\", ep+1, \"\\t Loss\\t\", loss)\n",
        "      loss_arr.append(loss)\n",
        "    accur = self.test_accuracy(layers, x_test, y_test)\n",
        "    print(\"Accuracy\\t\",accur, \"%\")\n",
        "    self.PlotError(loss_arr)\n",
        "\n",
        "\n",
        "###############################################################\n",
        "\n",
        "  def nesterov_gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, batch_size, eta, epochs, beta):\n",
        "    l = len(layers)\n",
        "    prev_w = []\n",
        "    prev_b = []\n",
        "    loss_arr = []\n",
        "    for i in range(l-1):\n",
        "      prev_w.append(np.zeros(self.W[i].shape))\n",
        "      prev_b.append(np.zeros(self.B[i].shape))\n",
        "\n",
        "    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)\n",
        "    \n",
        "    for ep in range(epochs):\n",
        "      for j in range(len(x_batch)):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        # print(\"y_hat\", ac[len(ac)-1])\n",
        "        for i in range(l-1):\n",
        "          self.W[i] += -beta * prev_w[i]\n",
        "          self.B[i] += -beta * prev_b[i]\n",
        "        preac, ac = self.forward(xb, layers)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac)\n",
        "        # print(\"grad_w\", grad_w)\n",
        "        for i in range(l-1):\n",
        "          prev_w[i] = beta * prev_w[i] + grad_w[l-i-2]\n",
        "          prev_b[i] = beta * prev_b[i] + grad_b[l-i-2]\n",
        "          self.W[i] += -eta * prev_w[i]\n",
        "          self.B[i] += -eta * prev_b[i]\n",
        "          # prev_w[i] = self.W[i]\n",
        "          # prev_b[i] = self.B[i]\n",
        "      loss = self.cross_entropy(yb, ac[len(ac)-1])\n",
        "      print(\"Iteration No : \\t\", ep+1, \"\\t Loss\\t\", loss)\n",
        "      loss_arr.append(loss)\n",
        "    accur = self.test_accuracy(layers, x_test, y_test)\n",
        "    print(\"Accuracy\\t\",accur, \"%\")\n",
        "    self.PlotError(loss_arr)\n",
        "\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "  def test_accuracy(self, layers, x, y):\n",
        "    preac, ac = self.forward(x, layers)\n",
        "    y_pred = ac[len(ac)-1]\n",
        "    err_count = 0\n",
        "    for i in range(y_pred.shape[0]):\n",
        "      maxval = -(math.inf)\n",
        "      maxind= -1\n",
        "      for j in range(y_pred.shape[1]): \n",
        "        if maxval < y_pred[i][j]:\n",
        "          maxval = y_pred[i][j]\n",
        "          maxind = j\n",
        "      if maxind != y[i]:\n",
        "        err_count = err_count + 1\n",
        "    return ((y.shape[0] - err_count)/y.shape[0])*100\n",
        "      \n",
        "###############################################################\n",
        "  def PlotError(self, ErrorSum):\n",
        "    Iter = [] \n",
        "    for i in range(len(ErrorSum)):\n",
        "      Iter.append(i)\n",
        "    plt.scatter(Iter,ErrorSum)\n",
        "    plt.title('Error v/s Iteration')\n",
        "    plt.xlabel('Error')\n",
        "    plt.ylabel('No of Iterations')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "eElQPRNABGNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch Gradient Descent"
      ],
      "metadata": {
        "id": "RbhfxkM7dyRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 10\n",
        "eta = 0.01\n",
        "layers = [784, 256, 10]\n",
        "no_of_classes = 10\n",
        "nn = NN(layers)\n",
        "batch_size = 60\n",
        "nn.batch_grad_descent(x_train, y_train, x_test, y_test, no_of_classes, layers, eta, batch_size, epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "zxrVj3Aw9VH0",
        "outputId": "f7923d5d-a2e6-421e-b459-1b89977722a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No : \t 1 \t Loss\t 0.8269711494457412\n",
            "Iteration No : \t 2 \t Loss\t 0.6688207955466299\n",
            "Iteration No : \t 3 \t Loss\t 0.5857252480798575\n",
            "Iteration No : \t 4 \t Loss\t 0.5456906733382101\n",
            "Iteration No : \t 5 \t Loss\t 0.5178528081101066\n",
            "Iteration No : \t 6 \t Loss\t 0.49002493018857923\n",
            "Iteration No : \t 7 \t Loss\t 0.4648499499037018\n",
            "Iteration No : \t 8 \t Loss\t 0.4414167477708107\n",
            "Iteration No : \t 9 \t Loss\t 0.42097554640624796\n",
            "Iteration No : \t 10 \t Loss\t 0.4036760368112359\n",
            "Accuracy\t 87.33999999999999 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla Gradient Descent"
      ],
      "metadata": {
        "id": "AlEYWCg0mtLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 400\n",
        "eta = 0.000001\n",
        "layers = [784, 128, 10]\n",
        "no_of_classes = 10\n",
        "nn = NN(layers)\n",
        "nn.gradient_descent(x_train, y_train,x_test, y_test, no_of_classes, layers, eta, epoch)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAnDgkWYWC-d",
        "outputId": "228048f4-79ee-4554-f05e-881d24a576fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No : \t 1 \t Loss\t 3.6367472029986976\n",
            "Iteration No : \t 2 \t Loss\t 3.3584382918548052\n",
            "Iteration No : \t 3 \t Loss\t 3.3244621090412174\n",
            "Iteration No : \t 4 \t Loss\t 3.3213545197322087\n",
            "Iteration No : \t 5 \t Loss\t 3.321128341169983\n",
            "Iteration No : \t 6 \t Loss\t 3.3211077157452062\n",
            "Iteration No : \t 7 \t Loss\t 3.321099490407265\n",
            "Iteration No : \t 8 \t Loss\t 3.3210919709125317\n",
            "Iteration No : \t 9 \t Loss\t 3.3210844910537447\n",
            "Iteration No : \t 10 \t Loss\t 3.321077013459607\n",
            "Iteration No : \t 11 \t Loss\t 3.321069536032354\n",
            "Iteration No : \t 12 \t Loss\t 3.321062058646246\n",
            "Iteration No : \t 13 \t Loss\t 3.3210545812855576\n",
            "Iteration No : \t 14 \t Loss\t 3.3210471039407463\n",
            "Iteration No : \t 15 \t Loss\t 3.3210396266027002\n",
            "Iteration No : \t 16 \t Loss\t 3.321032149262224\n",
            "Iteration No : \t 17 \t Loss\t 3.321024671910198\n",
            "Iteration No : \t 18 \t Loss\t 3.321017194537553\n",
            "Iteration No : \t 19 \t Loss\t 3.321009717135053\n",
            "Iteration No : \t 20 \t Loss\t 3.3210022396936467\n",
            "Iteration No : \t 21 \t Loss\t 3.320994762204133\n",
            "Iteration No : \t 22 \t Loss\t 3.320987284657302\n",
            "Iteration No : \t 23 \t Loss\t 3.320979807044228\n",
            "Iteration No : \t 24 \t Loss\t 3.3209723293556443\n",
            "Iteration No : \t 25 \t Loss\t 3.320964851582485\n",
            "Iteration No : \t 26 \t Loss\t 3.320957373715537\n",
            "Iteration No : \t 27 \t Loss\t 3.320949895745725\n",
            "Iteration No : \t 28 \t Loss\t 3.320942417663907\n",
            "Iteration No : \t 29 \t Loss\t 3.3209349394610284\n",
            "Iteration No : \t 30 \t Loss\t 3.320927461127882\n",
            "Iteration No : \t 31 \t Loss\t 3.320919982655397\n",
            "Iteration No : \t 32 \t Loss\t 3.320912504034343\n",
            "Iteration No : \t 33 \t Loss\t 3.32090502525567\n",
            "Iteration No : \t 34 \t Loss\t 3.320897546310289\n",
            "Iteration No : \t 35 \t Loss\t 3.3208900671890524\n",
            "Iteration No : \t 36 \t Loss\t 3.3208825878827395\n",
            "Iteration No : \t 37 \t Loss\t 3.3208751083823977\n",
            "Iteration No : \t 38 \t Loss\t 3.3208676286788084\n",
            "Iteration No : \t 39 \t Loss\t 3.3208601487629084\n",
            "Iteration No : \t 40 \t Loss\t 3.320852668625477\n",
            "Iteration No : \t 41 \t Loss\t 3.3208451882574517\n",
            "Iteration No : \t 42 \t Loss\t 3.320837707649747\n",
            "Iteration No : \t 43 \t Loss\t 3.320830226793225\n",
            "Iteration No : \t 44 \t Loss\t 3.32082274567873\n",
            "Iteration No : \t 45 \t Loss\t 3.3208152642972255\n",
            "Iteration No : \t 46 \t Loss\t 3.320807782639421\n",
            "Iteration No : \t 47 \t Loss\t 3.3208003006964035\n",
            "Iteration No : \t 48 \t Loss\t 3.320792818458949\n",
            "Iteration No : \t 49 \t Loss\t 3.3207853359179196\n",
            "Iteration No : \t 50 \t Loss\t 3.320777853064249\n",
            "Iteration No : \t 51 \t Loss\t 3.320770369888852\n",
            "Iteration No : \t 52 \t Loss\t 3.3207628863825476\n",
            "Iteration No : \t 53 \t Loss\t 3.3207554025361725\n",
            "Iteration No : \t 54 \t Loss\t 3.3207479183407425\n",
            "Iteration No : \t 55 \t Loss\t 3.3207404337870035\n",
            "Iteration No : \t 56 \t Loss\t 3.320732948865937\n",
            "Iteration No : \t 57 \t Loss\t 3.3207254635683507\n",
            "Iteration No : \t 58 \t Loss\t 3.320717977885118\n",
            "Iteration No : \t 59 \t Loss\t 3.320710491807226\n",
            "Iteration No : \t 60 \t Loss\t 3.3207030053254423\n",
            "Iteration No : \t 61 \t Loss\t 3.320695518430723\n",
            "Iteration No : \t 62 \t Loss\t 3.320688031113906\n",
            "Iteration No : \t 63 \t Loss\t 3.3206805433659032\n",
            "Iteration No : \t 64 \t Loss\t 3.3206730551775894\n",
            "Iteration No : \t 65 \t Loss\t 3.3206655665397884\n",
            "Iteration No : \t 66 \t Loss\t 3.3206580774434125\n",
            "Iteration No : \t 67 \t Loss\t 3.3206505878793187\n",
            "Iteration No : \t 68 \t Loss\t 3.3206430978384494\n",
            "Iteration No : \t 69 \t Loss\t 3.3206356073115737\n",
            "Iteration No : \t 70 \t Loss\t 3.320628116289634\n",
            "Iteration No : \t 71 \t Loss\t 3.3206206247635546\n",
            "Iteration No : \t 72 \t Loss\t 3.3206131327240946\n",
            "Iteration No : \t 73 \t Loss\t 3.3206056401622224\n",
            "Iteration No : \t 74 \t Loss\t 3.3205981470686816\n",
            "Iteration No : \t 75 \t Loss\t 3.320590653434555\n",
            "Iteration No : \t 76 \t Loss\t 3.320583159250481\n",
            "Iteration No : \t 77 \t Loss\t 3.3205756645074564\n",
            "Iteration No : \t 78 \t Loss\t 3.3205681691963256\n",
            "Iteration No : \t 79 \t Loss\t 3.320560673307972\n",
            "Iteration No : \t 80 \t Loss\t 3.3205531768332244\n",
            "Iteration No : \t 81 \t Loss\t 3.3205456797630006\n",
            "Iteration No : \t 82 \t Loss\t 3.3205381820881152\n",
            "Iteration No : \t 83 \t Loss\t 3.320530683799391\n",
            "Iteration No : \t 84 \t Loss\t 3.3205231848877546\n",
            "Iteration No : \t 85 \t Loss\t 3.3205156853441276\n",
            "Iteration No : \t 86 \t Loss\t 3.3205081851591607\n",
            "Iteration No : \t 87 \t Loss\t 3.3205006843239446\n",
            "Iteration No : \t 88 \t Loss\t 3.3204931828291695\n",
            "Iteration No : \t 89 \t Loss\t 3.3204856806658243\n",
            "Iteration No : \t 90 \t Loss\t 3.3204781778245804\n",
            "Iteration No : \t 91 \t Loss\t 3.3204706742964505\n",
            "Iteration No : \t 92 \t Loss\t 3.320463170072221\n",
            "Iteration No : \t 93 \t Loss\t 3.3204556651427044\n",
            "Iteration No : \t 94 \t Loss\t 3.3204481594988575\n",
            "Iteration No : \t 95 \t Loss\t 3.320440653131453\n",
            "Iteration No : \t 96 \t Loss\t 3.3204331460312537\n",
            "Iteration No : \t 97 \t Loss\t 3.320425638189233\n",
            "Iteration No : \t 98 \t Loss\t 3.3204181295961366\n",
            "Iteration No : \t 99 \t Loss\t 3.320410620242864\n",
            "Iteration No : \t 100 \t Loss\t 3.320403110120235\n",
            "Iteration No : \t 101 \t Loss\t 3.32039559921911\n",
            "Iteration No : \t 102 \t Loss\t 3.320388087530252\n",
            "Iteration No : \t 103 \t Loss\t 3.3203805750445223\n",
            "Iteration No : \t 104 \t Loss\t 3.320373061752735\n",
            "Iteration No : \t 105 \t Loss\t 3.3203655476457516\n",
            "Iteration No : \t 106 \t Loss\t 3.3203580327143993\n",
            "Iteration No : \t 107 \t Loss\t 3.32035051694942\n",
            "Iteration No : \t 108 \t Loss\t 3.3203430003417105\n",
            "Iteration No : \t 109 \t Loss\t 3.320335482882105\n",
            "Iteration No : \t 110 \t Loss\t 3.3203279645613604\n",
            "Iteration No : \t 111 \t Loss\t 3.3203204453702586\n",
            "Iteration No : \t 112 \t Loss\t 3.3203129252996786\n",
            "Iteration No : \t 113 \t Loss\t 3.3203054043403846\n",
            "Iteration No : \t 114 \t Loss\t 3.3202978824832727\n",
            "Iteration No : \t 115 \t Loss\t 3.3202903597190425\n",
            "Iteration No : \t 116 \t Loss\t 3.320282836038507\n",
            "Iteration No : \t 117 \t Loss\t 3.3202753114325163\n",
            "Iteration No : \t 118 \t Loss\t 3.3202677858918004\n",
            "Iteration No : \t 119 \t Loss\t 3.320260259407246\n",
            "Iteration No : \t 120 \t Loss\t 3.320252731969507\n",
            "Iteration No : \t 121 \t Loss\t 3.3202452035694194\n",
            "Iteration No : \t 122 \t Loss\t 3.3202376741979256\n",
            "Iteration No : \t 123 \t Loss\t 3.320230143845634\n",
            "Iteration No : \t 124 \t Loss\t 3.320222612503334\n",
            "Iteration No : \t 125 \t Loss\t 3.3202150801618444\n",
            "Iteration No : \t 126 \t Loss\t 3.320207546811949\n",
            "Iteration No : \t 127 \t Loss\t 3.3202000124444244\n",
            "Iteration No : \t 128 \t Loss\t 3.320192477049917\n",
            "Iteration No : \t 129 \t Loss\t 3.320184940619363\n",
            "Iteration No : \t 130 \t Loss\t 3.320177403143464\n",
            "Iteration No : \t 131 \t Loss\t 3.3201698646129683\n",
            "Iteration No : \t 132 \t Loss\t 3.3201623250186487\n",
            "Iteration No : \t 133 \t Loss\t 3.3201547843511046\n",
            "Iteration No : \t 134 \t Loss\t 3.3201472426013705\n",
            "Iteration No : \t 135 \t Loss\t 3.320139699759964\n",
            "Iteration No : \t 136 \t Loss\t 3.3201321558177344\n",
            "Iteration No : \t 137 \t Loss\t 3.3201246107653763\n",
            "Iteration No : \t 138 \t Loss\t 3.32011706459365\n",
            "Iteration No : \t 139 \t Loss\t 3.320109517293254\n",
            "Iteration No : \t 140 \t Loss\t 3.320101968855027\n",
            "Iteration No : \t 141 \t Loss\t 3.3200944192695485\n",
            "Iteration No : \t 142 \t Loss\t 3.3200868685275915\n",
            "Iteration No : \t 143 \t Loss\t 3.3200793166199225\n",
            "Iteration No : \t 144 \t Loss\t 3.320071763537291\n",
            "Iteration No : \t 145 \t Loss\t 3.3200642092702215\n",
            "Iteration No : \t 146 \t Loss\t 3.320056653809624\n",
            "Iteration No : \t 147 \t Loss\t 3.32004909714611\n",
            "Iteration No : \t 148 \t Loss\t 3.3200415392703864\n",
            "Iteration No : \t 149 \t Loss\t 3.3200339801731906\n",
            "Iteration No : \t 150 \t Loss\t 3.3200264198451284\n",
            "Iteration No : \t 151 \t Loss\t 3.3200188582770007\n",
            "Iteration No : \t 152 \t Loss\t 3.320011295459455\n",
            "Iteration No : \t 153 \t Loss\t 3.320003731383093\n",
            "Iteration No : \t 154 \t Loss\t 3.319996166038713\n",
            "Iteration No : \t 155 \t Loss\t 3.3199885994169613\n",
            "Iteration No : \t 156 \t Loss\t 3.319981031508404\n",
            "Iteration No : \t 157 \t Loss\t 3.319973462303854\n",
            "Iteration No : \t 158 \t Loss\t 3.3199658917939\n",
            "Iteration No : \t 159 \t Loss\t 3.319958319969179\n",
            "Iteration No : \t 160 \t Loss\t 3.319950746820404\n",
            "Iteration No : \t 161 \t Loss\t 3.3199431723382373\n",
            "Iteration No : \t 162 \t Loss\t 3.3199355965131985\n",
            "Iteration No : \t 163 \t Loss\t 3.3199280193360736\n",
            "Iteration No : \t 164 \t Loss\t 3.319920440797374\n",
            "Iteration No : \t 165 \t Loss\t 3.319912860887873\n",
            "Iteration No : \t 166 \t Loss\t 3.319905279598111\n",
            "Iteration No : \t 167 \t Loss\t 3.3198976969186322\n",
            "Iteration No : \t 168 \t Loss\t 3.319890112840279\n",
            "Iteration No : \t 169 \t Loss\t 3.3198825273534798\n",
            "Iteration No : \t 170 \t Loss\t 3.3198749404489147\n",
            "Iteration No : \t 171 \t Loss\t 3.3198673521171003\n",
            "Iteration No : \t 172 \t Loss\t 3.319859762348868\n",
            "Iteration No : \t 173 \t Loss\t 3.3198521711345372\n",
            "Iteration No : \t 174 \t Loss\t 3.3198445784648642\n",
            "Iteration No : \t 175 \t Loss\t 3.31983698433043\n",
            "Iteration No : \t 176 \t Loss\t 3.319829388721715\n",
            "Iteration No : \t 177 \t Loss\t 3.3198217916294275\n",
            "Iteration No : \t 178 \t Loss\t 3.3198141930440377\n",
            "Iteration No : \t 179 \t Loss\t 3.319806592956181\n",
            "Iteration No : \t 180 \t Loss\t 3.3197989913563757\n",
            "Iteration No : \t 181 \t Loss\t 3.3197913882352204\n",
            "Iteration No : \t 182 \t Loss\t 3.319783783583206\n",
            "Iteration No : \t 183 \t Loss\t 3.3197761773909606\n",
            "Iteration No : \t 184 \t Loss\t 3.319768569648917\n",
            "Iteration No : \t 185 \t Loss\t 3.319760960347764\n",
            "Iteration No : \t 186 \t Loss\t 3.319753349477896\n",
            "Iteration No : \t 187 \t Loss\t 3.3197457370299084\n",
            "Iteration No : \t 188 \t Loss\t 3.3197381229942766\n",
            "Iteration No : \t 189 \t Loss\t 3.3197305073616024\n",
            "Iteration No : \t 190 \t Loss\t 3.3197228901223177\n",
            "Iteration No : \t 191 \t Loss\t 3.319715271266994\n",
            "Iteration No : \t 192 \t Loss\t 3.3197076507860444\n",
            "Iteration No : \t 193 \t Loss\t 3.319700028670048\n",
            "Iteration No : \t 194 \t Loss\t 3.3196924049094534\n",
            "Iteration No : \t 195 \t Loss\t 3.3196847794947257\n",
            "Iteration No : \t 196 \t Loss\t 3.319677152416381\n",
            "Iteration No : \t 197 \t Loss\t 3.319669523664864\n",
            "Iteration No : \t 198 \t Loss\t 3.3196618932306885\n",
            "Iteration No : \t 199 \t Loss\t 3.3196542611042075\n",
            "Iteration No : \t 200 \t Loss\t 3.319646627276091\n",
            "Iteration No : \t 201 \t Loss\t 3.3196389917365483\n",
            "Iteration No : \t 202 \t Loss\t 3.319631354476157\n",
            "Iteration No : \t 203 \t Loss\t 3.319623715485262\n",
            "Iteration No : \t 204 \t Loss\t 3.319616074754447\n",
            "Iteration No : \t 205 \t Loss\t 3.319608432273994\n",
            "Iteration No : \t 206 \t Loss\t 3.3196007880343714\n",
            "Iteration No : \t 207 \t Loss\t 3.3195931420260423\n",
            "Iteration No : \t 208 \t Loss\t 3.319585494239335\n",
            "Iteration No : \t 209 \t Loss\t 3.3195778446646975\n",
            "Iteration No : \t 210 \t Loss\t 3.3195701932924986\n",
            "Iteration No : \t 211 \t Loss\t 3.319562540113199\n",
            "Iteration No : \t 212 \t Loss\t 3.3195548851171512\n",
            "Iteration No : \t 213 \t Loss\t 3.319547228294633\n",
            "Iteration No : \t 214 \t Loss\t 3.31953956963604\n",
            "Iteration No : \t 215 \t Loss\t 3.3195319091319306\n",
            "Iteration No : \t 216 \t Loss\t 3.319524246772394\n",
            "Iteration No : \t 217 \t Loss\t 3.3195165825479953\n",
            "Iteration No : \t 218 \t Loss\t 3.3195089164489797\n",
            "Iteration No : \t 219 \t Loss\t 3.319501248465623\n",
            "Iteration No : \t 220 \t Loss\t 3.319493578588422\n",
            "Iteration No : \t 221 \t Loss\t 3.3194859068075275\n",
            "Iteration No : \t 222 \t Loss\t 3.3194782331134434\n",
            "Iteration No : \t 223 \t Loss\t 3.3194705574963113\n",
            "Iteration No : \t 224 \t Loss\t 3.3194628799465304\n",
            "Iteration No : \t 225 \t Loss\t 3.319455200454358\n",
            "Iteration No : \t 226 \t Loss\t 3.3194475190101307\n",
            "Iteration No : \t 227 \t Loss\t 3.31943983560404\n",
            "Iteration No : \t 228 \t Loss\t 3.3194321502264788\n",
            "Iteration No : \t 229 \t Loss\t 3.31942446286772\n",
            "Iteration No : \t 230 \t Loss\t 3.319416773517904\n",
            "Iteration No : \t 231 \t Loss\t 3.3194090821673736\n",
            "Iteration No : \t 232 \t Loss\t 3.3194013888063894\n",
            "Iteration No : \t 233 \t Loss\t 3.3193936934251003\n",
            "Iteration No : \t 234 \t Loss\t 3.3193859960138288\n",
            "Iteration No : \t 235 \t Loss\t 3.319378296562846\n",
            "Iteration No : \t 236 \t Loss\t 3.319370595062237\n",
            "Iteration No : \t 237 \t Loss\t 3.3193628915022866\n",
            "Iteration No : \t 238 \t Loss\t 3.319355185873301\n",
            "Iteration No : \t 239 \t Loss\t 3.3193474781653034\n",
            "Iteration No : \t 240 \t Loss\t 3.319339768368566\n",
            "Iteration No : \t 241 \t Loss\t 3.3193320564732947\n",
            "Iteration No : \t 242 \t Loss\t 3.319324342469605\n",
            "Iteration No : \t 243 \t Loss\t 3.3193166263477334\n",
            "Iteration No : \t 244 \t Loss\t 3.3193089080977725\n",
            "Iteration No : \t 245 \t Loss\t 3.3193011877099727\n",
            "Iteration No : \t 246 \t Loss\t 3.319293465174356\n",
            "Iteration No : \t 247 \t Loss\t 3.3192857404811416\n",
            "Iteration No : \t 248 \t Loss\t 3.319278013620491\n",
            "Iteration No : \t 249 \t Loss\t 3.3192702845824673\n",
            "Iteration No : \t 250 \t Loss\t 3.3192625533571234\n",
            "Iteration No : \t 251 \t Loss\t 3.3192548199346574\n",
            "Iteration No : \t 252 \t Loss\t 3.3192470843051645\n",
            "Iteration No : \t 253 \t Loss\t 3.3192393464587795\n",
            "Iteration No : \t 254 \t Loss\t 3.3192316063854332\n",
            "Iteration No : \t 255 \t Loss\t 3.3192238640753478\n",
            "Iteration No : \t 256 \t Loss\t 3.3192161195185093\n",
            "Iteration No : \t 257 \t Loss\t 3.319208372705061\n",
            "Iteration No : \t 258 \t Loss\t 3.3192006236248983\n",
            "Iteration No : \t 259 \t Loss\t 3.319192872268238\n",
            "Iteration No : \t 260 \t Loss\t 3.319185118625001\n",
            "Iteration No : \t 261 \t Loss\t 3.3191773626852696\n",
            "Iteration No : \t 262 \t Loss\t 3.319169604439048\n",
            "Iteration No : \t 263 \t Loss\t 3.319161843876256\n",
            "Iteration No : \t 264 \t Loss\t 3.3191540809870324\n",
            "Iteration No : \t 265 \t Loss\t 3.319146315761281\n",
            "Iteration No : \t 266 \t Loss\t 3.319138548189068\n",
            "Iteration No : \t 267 \t Loss\t 3.31913077826025\n",
            "Iteration No : \t 268 \t Loss\t 3.3191230059649124\n",
            "Iteration No : \t 269 \t Loss\t 3.3191152312928844\n",
            "Iteration No : \t 270 \t Loss\t 3.3191074542341945\n",
            "Iteration No : \t 271 \t Loss\t 3.319099674778808\n",
            "Iteration No : \t 272 \t Loss\t 3.319091892916573\n",
            "Iteration No : \t 273 \t Loss\t 3.3190841086375342\n",
            "Iteration No : \t 274 \t Loss\t 3.319076321931392\n",
            "Iteration No : \t 275 \t Loss\t 3.3190685327882714\n",
            "Iteration No : \t 276 \t Loss\t 3.319060741198001\n",
            "Iteration No : \t 277 \t Loss\t 3.319052947150404\n",
            "Iteration No : \t 278 \t Loss\t 3.3190451506353993\n",
            "Iteration No : \t 279 \t Loss\t 3.3190373516428577\n",
            "Iteration No : \t 280 \t Loss\t 3.3190295501626212\n",
            "Iteration No : \t 281 \t Loss\t 3.3190217461845504\n",
            "Iteration No : \t 282 \t Loss\t 3.3190139396984835\n",
            "Iteration No : \t 283 \t Loss\t 3.3190061306942313\n",
            "Iteration No : \t 284 \t Loss\t 3.318998319161658\n",
            "Iteration No : \t 285 \t Loss\t 3.318990505090575\n",
            "Iteration No : \t 286 \t Loss\t 3.318982688470695\n",
            "Iteration No : \t 287 \t Loss\t 3.3189748692919174\n",
            "Iteration No : \t 288 \t Loss\t 3.3189670475439232\n",
            "Iteration No : \t 289 \t Loss\t 3.3189592232165563\n",
            "Iteration No : \t 290 \t Loss\t 3.3189513962996\n",
            "Iteration No : \t 291 \t Loss\t 3.318943566782775\n",
            "Iteration No : \t 292 \t Loss\t 3.318935734655801\n",
            "Iteration No : \t 293 \t Loss\t 3.3189278999084455\n",
            "Iteration No : \t 294 \t Loss\t 3.318920062530391\n",
            "Iteration No : \t 295 \t Loss\t 3.3189122225114214\n",
            "Iteration No : \t 296 \t Loss\t 3.3189043798411695\n",
            "Iteration No : \t 297 \t Loss\t 3.318896534509371\n",
            "Iteration No : \t 298 \t Loss\t 3.318888686505637\n",
            "Iteration No : \t 299 \t Loss\t 3.318880835819797\n",
            "Iteration No : \t 300 \t Loss\t 3.3188729824414107\n",
            "Iteration No : \t 301 \t Loss\t 3.318865126360104\n",
            "Iteration No : \t 302 \t Loss\t 3.31885726756557\n",
            "Iteration No : \t 303 \t Loss\t 3.31884940604737\n",
            "Iteration No : \t 304 \t Loss\t 3.3188415417952775\n",
            "Iteration No : \t 305 \t Loss\t 3.3188336747987166\n",
            "Iteration No : \t 306 \t Loss\t 3.318825805047455\n",
            "Iteration No : \t 307 \t Loss\t 3.31881793253098\n",
            "Iteration No : \t 308 \t Loss\t 3.318810057238954\n",
            "Iteration No : \t 309 \t Loss\t 3.3188021791608335\n",
            "Iteration No : \t 310 \t Loss\t 3.318794298286251\n",
            "Iteration No : \t 311 \t Loss\t 3.3187864146047645\n",
            "Iteration No : \t 312 \t Loss\t 3.318778528105928\n",
            "Iteration No : \t 313 \t Loss\t 3.3187706387791494\n",
            "Iteration No : \t 314 \t Loss\t 3.31876274661408\n",
            "Iteration No : \t 315 \t Loss\t 3.318754851600163\n",
            "Iteration No : \t 316 \t Loss\t 3.318746953726922\n",
            "Iteration No : \t 317 \t Loss\t 3.318739052983784\n",
            "Iteration No : \t 318 \t Loss\t 3.318731149360238\n",
            "Iteration No : \t 319 \t Loss\t 3.318723242845828\n",
            "Iteration No : \t 320 \t Loss\t 3.3187153334298762\n",
            "Iteration No : \t 321 \t Loss\t 3.318707421101936\n",
            "Iteration No : \t 322 \t Loss\t 3.318699505851343\n",
            "Iteration No : \t 323 \t Loss\t 3.318691587667589\n",
            "Iteration No : \t 324 \t Loss\t 3.318683666540052\n",
            "Iteration No : \t 325 \t Loss\t 3.3186757424580997\n",
            "Iteration No : \t 326 \t Loss\t 3.3186678154111298\n",
            "Iteration No : \t 327 \t Loss\t 3.318659885388524\n",
            "Iteration No : \t 328 \t Loss\t 3.318651952379616\n",
            "Iteration No : \t 329 \t Loss\t 3.3186440163737663\n",
            "Iteration No : \t 330 \t Loss\t 3.3186360773603343\n",
            "Iteration No : \t 331 \t Loss\t 3.3186281353286233\n",
            "Iteration No : \t 332 \t Loss\t 3.318620190267916\n",
            "Iteration No : \t 333 \t Loss\t 3.318612242167536\n",
            "Iteration No : \t 334 \t Loss\t 3.318604291016846\n",
            "Iteration No : \t 335 \t Loss\t 3.318596336804965\n",
            "Iteration No : \t 336 \t Loss\t 3.3185883795212914\n",
            "Iteration No : \t 337 \t Loss\t 3.318580419154987\n",
            "Iteration No : \t 338 \t Loss\t 3.318572455695417\n",
            "Iteration No : \t 339 \t Loss\t 3.3185644891316795\n",
            "Iteration No : \t 340 \t Loss\t 3.318556519453013\n",
            "Iteration No : \t 341 \t Loss\t 3.3185485466486977\n",
            "Iteration No : \t 342 \t Loss\t 3.3185405707078313\n",
            "Iteration No : \t 343 \t Loss\t 3.3185325916196247\n",
            "Iteration No : \t 344 \t Loss\t 3.3185246093733367\n",
            "Iteration No : \t 345 \t Loss\t 3.3185166239579886\n",
            "Iteration No : \t 346 \t Loss\t 3.3185086353627535\n",
            "Iteration No : \t 347 \t Loss\t 3.3185006435767965\n",
            "Iteration No : \t 348 \t Loss\t 3.3184926485892663\n",
            "Iteration No : \t 349 \t Loss\t 3.318484650389169\n",
            "Iteration No : \t 350 \t Loss\t 3.318476648965665\n",
            "Iteration No : \t 351 \t Loss\t 3.3184686443078353\n",
            "Iteration No : \t 352 \t Loss\t 3.318460636404661\n",
            "Iteration No : \t 353 \t Loss\t 3.3184526252453326\n",
            "Iteration No : \t 354 \t Loss\t 3.3184446108187426\n",
            "Iteration No : \t 355 \t Loss\t 3.31843659311406\n",
            "Iteration No : \t 356 \t Loss\t 3.3184285721201796\n",
            "Iteration No : \t 357 \t Loss\t 3.3184205478261246\n",
            "Iteration No : \t 358 \t Loss\t 3.3184125202209867\n",
            "Iteration No : \t 359 \t Loss\t 3.3184044892936226\n",
            "Iteration No : \t 360 \t Loss\t 3.318396455033052\n",
            "Iteration No : \t 361 \t Loss\t 3.318388417428146\n",
            "Iteration No : \t 362 \t Loss\t 3.318380376468001\n",
            "Iteration No : \t 363 \t Loss\t 3.318372332141345\n",
            "Iteration No : \t 364 \t Loss\t 3.3183642844371555\n",
            "Iteration No : \t 365 \t Loss\t 3.318356233344428\n",
            "Iteration No : \t 366 \t Loss\t 3.3183481788519043\n",
            "Iteration No : \t 367 \t Loss\t 3.318340120948538\n",
            "Iteration No : \t 368 \t Loss\t 3.3183320596230943\n",
            "Iteration No : \t 369 \t Loss\t 3.318323994864536\n",
            "Iteration No : \t 370 \t Loss\t 3.3183159266615236\n",
            "Iteration No : \t 371 \t Loss\t 3.3183078550030585\n",
            "Iteration No : \t 372 \t Loss\t 3.3182997798777527\n",
            "Iteration No : \t 373 \t Loss\t 3.3182917012745716\n",
            "Iteration No : \t 374 \t Loss\t 3.3182836191821354\n",
            "Iteration No : \t 375 \t Loss\t 3.3182755335892717\n",
            "Iteration No : \t 376 \t Loss\t 3.3182674444846625\n",
            "Iteration No : \t 377 \t Loss\t 3.318259351857088\n",
            "Iteration No : \t 378 \t Loss\t 3.3182512556953165\n",
            "Iteration No : \t 379 \t Loss\t 3.318243155987954\n",
            "Iteration No : \t 380 \t Loss\t 3.3182350527237117\n",
            "Iteration No : \t 381 \t Loss\t 3.3182269458912264\n",
            "Iteration No : \t 382 \t Loss\t 3.3182188354792466\n",
            "Iteration No : \t 383 \t Loss\t 3.318210721476277\n",
            "Iteration No : \t 384 \t Loss\t 3.3182026038710437\n",
            "Iteration No : \t 385 \t Loss\t 3.318194482652102\n",
            "Iteration No : \t 386 \t Loss\t 3.3181863578080946\n",
            "Iteration No : \t 387 \t Loss\t 3.3181782293275646\n",
            "Iteration No : \t 388 \t Loss\t 3.3181700971990984\n",
            "Iteration No : \t 389 \t Loss\t 3.318161961411244\n",
            "Iteration No : \t 390 \t Loss\t 3.3181538219525706\n",
            "Iteration No : \t 391 \t Loss\t 3.3181456788115384\n",
            "Iteration No : \t 392 \t Loss\t 3.3181375319766593\n",
            "Iteration No : \t 393 \t Loss\t 3.318129381436464\n",
            "Iteration No : \t 394 \t Loss\t 3.3181212271794607\n",
            "Iteration No : \t 395 \t Loss\t 3.3181130691939877\n",
            "Iteration No : \t 396 \t Loss\t 3.3181049074685873\n",
            "Iteration No : \t 397 \t Loss\t 3.318096741991698\n",
            "Iteration No : \t 398 \t Loss\t 3.318088572751706\n",
            "Iteration No : \t 399 \t Loss\t 3.318080399736925\n",
            "Iteration No : \t 400 \t Loss\t 3.3180722229358466\n",
            "Accuracy\t 12.389999999999999 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Momentum Gradient Descent"
      ],
      "metadata": {
        "id": "4u_RpwPLz8D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 10\n",
        "eta = 0.001\n",
        "beta = 0.1\n",
        "layers = [784, 128, 10]\n",
        "no_of_classes = 10\n",
        "batch_size = 1024\n",
        "nn = NN(layers)\n",
        "nn.momentum_grad_descent(x_train, y_train, x_test, y_test, no_of_classes, layers, batch_size, eta, epoch, beta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "P4nDyLrYsbmR",
        "outputId": "cf58a9af-0c35-4cd1-9710-64c7e464e0e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No : \t 1 \t Loss\t 6.038900084408536\n",
            "Iteration No : \t 2 \t Loss\t 1.2846741767225596\n",
            "Iteration No : \t 3 \t Loss\t 1.0476759328135807\n",
            "Iteration No : \t 4 \t Loss\t 0.9429777276906198\n",
            "Iteration No : \t 5 \t Loss\t 0.8790600654248028\n",
            "Iteration No : \t 6 \t Loss\t 0.8428808988718357\n",
            "Iteration No : \t 7 \t Loss\t 0.695050985817123\n",
            "Iteration No : \t 8 \t Loss\t 0.7237840080037694\n",
            "Iteration No : \t 9 \t Loss\t 0.6632077578223967\n",
            "Iteration No : \t 10 \t Loss\t 0.6555642772618165\n",
            "Accuracy\t 83.2 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Netserov Gradient Descent"
      ],
      "metadata": {
        "id": "BYt1DK9YyXMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 10\n",
        "eta = 0.001\n",
        "beta = 0.001\n",
        "layers = [784, 128, 10]\n",
        "no_of_classes = 10\n",
        "batch_size = 1024\n",
        "nn = NN(layers)\n",
        "nn.nesterov_gradient_descent(x_train, y_train, x_test, y_test, no_of_classes, layers, batch_size, eta, epoch, beta)"
      ],
      "metadata": {
        "id": "n89-rmMueq27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "8c214388-7222-4a9f-9612-668008245092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration No : \t 1 \t Loss\t 1.3834266319775643\n",
            "Iteration No : \t 2 \t Loss\t 0.9891439289292124\n",
            "Iteration No : \t 3 \t Loss\t 0.8616428868812246\n",
            "Iteration No : \t 4 \t Loss\t 0.7519662102265232\n",
            "Iteration No : \t 5 \t Loss\t 0.6510561099688473\n",
            "Iteration No : \t 6 \t Loss\t 0.6543426004339035\n",
            "Iteration No : \t 7 \t Loss\t 0.6464551330056488\n",
            "Iteration No : \t 8 \t Loss\t 0.6279000632294789\n",
            "Iteration No : \t 9 \t Loss\t 0.6154881682187843\n",
            "Iteration No : \t 10 \t Loss\t 0.6034212078753164\n",
            "Accuracy\t 83.98 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}